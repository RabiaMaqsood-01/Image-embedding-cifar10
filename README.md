
## ğŸ¯ Objectives
- Generate robust image embeddings from CIFAR-10 using pre-trained CNNs.
- Improve embedding quality with fine-tuning and triplet loss.
- Evaluate retrieval performance using mean Precision@5 (mAP@5).
- Apply clustering (K-Means + t-SNE) for embedding visualization.
- Build a stacking ensemble model for combining multiple embeddings.
 ## ğŸ“‚ Project Structure
 cifar10-image-embedding/
â”œâ”€â”€ preprocessing.py # Dataset loading, normalization, splitting, augmentation
â”œâ”€â”€ densenet121_model.py # DenseNet121-based embedding model
â”œâ”€â”€ resnet50_model.py # ResNet50-based embedding model
â”œâ”€â”€ vgg16_model.py # VGG16-based embedding model
â”œâ”€â”€ stacking_ensemble.py # Combines embeddings via Stacking Ensemble Technique
â”œâ”€â”€ requirements.txt # Required Python packages
â”œâ”€â”€ README.md # Project description and instructions
â”œâ”€â”€ embeddings.csv # Saved DenseNet embeddings
â”œâ”€â”€ training_validation_loss.png # Loss graph image
â””â”€â”€ clustering_visualization.png # Embedding cluster visualization

## ğŸ–¼ï¸ Dataset
- **CIFAR-10**: 60,000 32x32 color images in 10 classes.
- Downloaded using: `keras.datasets.cifar10`

## ğŸ§  Models Used
- **DenseNet121** 
- **ResNet50**
- **VGG16**
- **Stacking Ensemble** 

## ğŸ“Š Evaluation Metric
- **mAP@5 (mean Average Precision at 5)**

## âš™ï¸ Requirements
Install the necessary libraries with:

## ğŸš€ How to Run
# Step 1: Preprocess and augment CIFAR-10 data
python preprocessing.py

# Step 2: Train individual models
python densenet121_model.py
python resnet50_model.py
python vgg16_model.py

# Step 3: Combine embeddings using stacking
python stacking_ensemble.py

## ğŸ§ª Results

| Model        | Training Accuracy | Test Accuracy | mAP@5 Score |
|--------------|-------------------|---------------|-------------|
| DenseNet121  | 85.48%            | 81.32%        | **83.71%**  |
| ResNet50     | 82.44%            | 79.15%        | -           |
| VGG16        | 85.54%            | 82.70%        | -           |
| Stacking     | 92.86%            | 87.45%        | -           |
